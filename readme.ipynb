{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSAI-HW1\n",
    "\n",
    "## Description\n",
    "\n",
    "I use the policy gradient (actor-critic) to model the stock state.\n",
    "Under the observation, I assume that `10-day moving average difference` and `5-20-day moving average` are related to the stock up and rise.\n",
    "Also, We have 5 stock states: `great down`, `down`, `steady`, `up`, `great up`\n",
    "Considered about this, taking this policy to train my model with the `Actor-Critic` NNs.\n",
    "\n",
    "Once we have the future stock state, we can simply take action.\n",
    "\n",
    "```\n",
    "Buying shares (or returning shares in shorting) in `up` or `great up`\n",
    "Selling shares (or shorting) in `great down`\n",
    "Holding shares in other states\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Install the dependency\n",
    "\n",
    "- In your virtual environment (you can use pyenv, pipenv, virtualenv, ..., etc.)\n",
    "\n",
    "```sh\n",
    "$ pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "- execute the script (you can use arguments to specify the files)\n",
    "\n",
    "```sh\n",
    "$ python3 trader.py [--training=training_data.csv] [--testing=testing_data.csv] [--output=output.csv]\n",
    "```\n",
    "\n",
    "## AUTHORS\n",
    "\n",
    "[rapirent](https://github.com/raprient)\n",
    "\n",
    "\n",
    "## LICENSE\n",
    "MIT@2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detail\n",
    "\n",
    "### ac.py\n",
    "我將Actor-Critic寫於ac.py中，裡面有兩個class宣告:`Actor`, `Critic`，使用tensorflow實現詳細算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "GAMMA = 0.9     # reward discount in TD error\n",
    "\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, n_actions, lr=0.001):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.a = tf.placeholder(tf.int32, None, \"act\")\n",
    "        self.td_error = tf.placeholder(tf.float32, None, \"td_error\")  # TD_error\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=20,    # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),    # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.acts_prob = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=n_actions,    # output units\n",
    "                activation=tf.nn.softmax,   # get action probabilities\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='acts_prob'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('exp_v'):\n",
    "            log_prob = tf.log(self.acts_prob[0, self.a])\n",
    "            self.exp_v = tf.reduce_mean(log_prob * self.td_error)  # advantage (TD_error) guided loss\n",
    "\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v)  # minimize(-exp_v) = maximize(exp_v)\n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        probs = self.sess.run(self.acts_prob, {self.s: s})   # get probabilities for all actions\n",
    "        return np.random.choice(np.arange(probs.shape[1]), p=probs.ravel())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr=0.01):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.v_ = tf.placeholder(tf.float32, [1, 1], \"v_next\")\n",
    "        self.r = tf.placeholder(tf.float32, None, 'r')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=20,  # number of hidden units\n",
    "                activation=tf.nn.relu,  # None\n",
    "                # have to be linear to make sure the convergence of actor.\n",
    "                # But linear approximator seems hardly learns the correct Q.\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = self.r + GAMMA * self.v_ - self.v\n",
    "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### env.py\n",
    "由於我的假設是使用policy-gradient-based的NN(Actor-Critic)來預測隔日股市的變化，由預測的變化做出相對的動作(買入、賣出、不動作)\n",
    "\n",
    "- 為此，我預設了兩個策略：\n",
    "    1. 5-20均線策略：將5日平均及20日平均算出，並計算其交叉前是5日均線較高還是20日均線較低。\n",
    "        - 若是5日均線較低，代表將會上漲，隔日預測值應為大漲 (4)\n",
    "        - 反之，則應預測為大跌(0)\n",
    "    2. 10日平均線變化策略：計算今日與昨日的十日平均變化，並計算訓練資料的每日10日平均標準差\n",
    "        - 若是變化大於`(標準差/4) - 全十日平均值的平均`，則應為小漲(3)\n",
    "        - 若是變化小於`(標準差/4) - 全十日平均值的平均`，則應為小跌(1)\n",
    "        - 其餘則為平穩不變化(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plus = lambda value: value if value > 0 else 0\n",
    "\n",
    "class Env:\n",
    "\n",
    "    def __init__(self, data=None, period=10):\n",
    "        self._DATA = data\n",
    "        self._PERIOD = period\n",
    "        self._SHORT_PERIOD = 5\n",
    "        self._LONG_PERIOD = 20\n",
    "        self.train_mean = None\n",
    "        self.train_quantile = None\n",
    "        # XXX\n",
    "        self.error_count = [0,0,0]\n",
    "\n",
    "    def load_data(self, data):\n",
    "        self._DATA = data\n",
    "\n",
    "    def data_len(self):\n",
    "        return len(self._DATA['close'])\n",
    "\n",
    "    def training_preprocess(self):\n",
    "        all_moving_average = []\n",
    "        all_moving_average_diff = []\n",
    "        for index in range(self.data_len()):\n",
    "            all_moving_average.append(np.mean([ self._DATA['open'][plus(index - i)] for i in range(self._PERIOD) ] ) - self._DATA['open'][0])\n",
    "        for index in range(1,self.data_len()):\n",
    "            all_moving_average_diff.append(all_moving_average[index] - all_moving_average[index - 1])\n",
    "        print(all_moving_average_diff)\n",
    "        self.train_quantile =  (np.percentile(all_moving_average_diff, 30), np.percentile(all_moving_average_diff, 40),\n",
    "                            np.percentile(all_moving_average_diff, 60), np.percentile(all_moving_average_diff, 70))\n",
    "        self.train_mean = np.mean(all_moving_average)\n",
    "        self.avg_diff = np.mean(all_moving_average_diff)\n",
    "        error = []\n",
    "        for index in all_moving_average_diff:\n",
    "            error.append(index- self.avg_diff)\n",
    "        squaredError = []\n",
    "        for index in error:\n",
    "            squaredError.append(index ** 2)\n",
    "\n",
    "        self.avg_diff_mse = np.mean(squaredError) ** 0.5\n",
    "        self.avg_diff_std = np.std(all_moving_average_diff)\n",
    "        self.avg_diff_quantile = (self.avg_diff_std,\n",
    "                                  self.avg_diff_std/4 - self.avg_diff,\n",
    "                                  (-self.avg_diff_std/4) + self.avg_diff,\n",
    "                                  -self.avg_diff_std)\n",
    "\n",
    "    def reset(self):\n",
    "        long_moving_average = np.mean([ self._DATA['close'][plus(0 - i)] for i in range(self._LONG_PERIOD) ])\n",
    "        short_moving_average = np.mean([ self._DATA['close'][plus(0 - i)] for i in range(self._SHORT_PERIOD) ])\n",
    "        moving_average = np.mean([ self._DATA['open'][plus(0 - i)] for i in range(self._PERIOD) ]) - self._DATA['open'][0]\n",
    "        cross_diff = long_moving_average - short_moving_average\n",
    "        state = np.array([0, 0, 0, cross_diff, cross_diff])\n",
    "        return (long_moving_average, short_moving_average, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 以此，我將每次的狀態設為`十日平均`, `十日平均變化`, `當日情況`, `今日5-20均線差值`, `昨日5-20均線差值`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_env(self, today, last_average, last_long_avg, last_short_avg):\n",
    "\n",
    "        moving_average = np.mean( [self._DATA['open'][plus(today - i)] for i in range(self._PERIOD)] ) - self._DATA['open'][today]\n",
    "        long_moving_average = np.mean([ self._DATA['close'][plus(today - i)] for i in range(self._LONG_PERIOD) ])\n",
    "        short_moving_average = np.mean([ self._DATA['close'][plus(today - i)] for i in range(self._SHORT_PERIOD) ])\n",
    "        moving_average_diff = moving_average - last_average\n",
    "        current_cross_diff = long_moving_average - short_moving_average\n",
    "        last_cross_diff = last_long_avg - last_short_avg\n",
    "\n",
    "        if current_cross_diff * last_cross_diff < 0:\n",
    "            if short_moving_average >= long_moving_average:\n",
    "                action_real = 4\n",
    "            else:\n",
    "                action_real = 0\n",
    "        else:\n",
    "            if moving_average_diff > self.avg_diff_quantile[0]:\n",
    "                action_real = 3\n",
    "            # elif moving_average_diff < self.avg_diff_quantile[3]:\n",
    "            #     action_real = 0\n",
    "            elif moving_average_diff < self.avg_diff_quantile[2]:\n",
    "                action_real = 1\n",
    "            else:\n",
    "                action_real = 2\n",
    "        last_long_avg = long_moving_average\n",
    "        last_short_avg = short_moving_average\n",
    "        return (last_long_avg, last_short_avg, np.array([moving_average, moving_average_diff, action_real, current_cross_diff, last_cross_diff]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reward\n",
    "\n",
    "- 而對於reward值，由於我們是在「模擬」股市環境，所以將預測出來的值與`get_env`計算出來的實際狀態做比較，假如差異者，則給予懲罰（負的reward），反之則給予獎勵（正的reward）\n",
    "    - 如果相同，則reward為20\n",
    "    - 如果猜測出的漲跌狀態差值大於0，則reward為-100\n",
    "    - 若是猜測漲跌狀態差異不大，但仍有差異者，則reward為-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def step(self, today, action, action_real, last_average, last_long_avg, last_short_avg):\n",
    "\n",
    "        last_long_avg_, last_short_avg_, n_state = self.get_env(today, last_average, last_long_avg, last_short_avg)\n",
    "        if action_real == action:\n",
    "            reward = 20\n",
    "            self.error_count[1] += 1\n",
    "        elif action_real - action > 1 or action_real - action < -1:\n",
    "            reward = -100\n",
    "            self.error_count[0] += 1\n",
    "        else:\n",
    "            reward = -50\n",
    "            self.error_count[2] += 1\n",
    "\n",
    "        return (last_long_avg_, last_short_avg_, reward, n_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stocktrader.py\n",
    "- 為了模組化及方便計算，我於`stocktrader.py`腳本撰寫`StockTrader`類別，為了能夠明確知道各數值代表的意義而自定義了ACTION_LIST字典，也邊寫了一`StockValueError`，希望能夠在進行動作不合法時拋出特定錯誤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_LIST = {'BUY': 1, 'IDLE': 0, 'SELL': -1}\n",
    "\n",
    "class StockValueError(Exception):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def __str__(self):\n",
    "        return repr('WRONG AT ' + self.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `StockTrader`類別有兩個比較特別的方法：`predict_action()`和`reaction()`\n",
    "\n",
    "    - predict_action()方法根據預測狀態值(0~4)來做出相對應的動作\n",
    "        - 1: 購買\n",
    "        - 0: 不動作\n",
    "        - -1: 賣出\n",
    "    - 做出動作後，可以使用reaction()方法計算此動作能夠帶來的收益或損失，並且改變StockTrader的一自變數:`self.current_state`，此狀態數值與動作值並不一樣，但他們都是介於-1~1的整數值\n",
    "        - current_state = 0時代表不擁有股票\n",
    "        - 值為1時代表擁有股票\n",
    "        - 值為-1時代表空頭股票（出借）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTrader():\n",
    "\n",
    "    def __init__(self, init_state = 0, data=None):\n",
    "        self.current_state = init_state\n",
    "        self._state = [-1, 0, 1]\n",
    "        self.money = 0\n",
    "        self._DATA = data\n",
    "\n",
    "    def load_data(self, data):\n",
    "        self._DATA = data\n",
    "\n",
    "    def data_len(self):\n",
    "        return len(self._DATA['open'])\n",
    "\n",
    "    def reset(self):\n",
    "        self.record = 0\n",
    "        self.current_state = 0\n",
    "\n",
    "    def set_state(self, state):\n",
    "        self.current_state = state\n",
    "\n",
    "    def predict_action(self, trend, i):\n",
    "        if self.current_state == 0:\n",
    "            if trend > 2:\n",
    "                # up\n",
    "                # BUY\n",
    "                action = ACTION_LIST['BUY']\n",
    "            if trend < 2:\n",
    "                # large down\n",
    "                action = ACTION_LIST['SELL']\n",
    "            else:\n",
    "                action = ACTION_LIST['IDLE']\n",
    "        elif self.current_state == 1:\n",
    "            if trend < 2:\n",
    "                # large down\n",
    "                # SOLD!\n",
    "                action = ACTION_LIST['SELL']\n",
    "            else:\n",
    "                action = ACTION_LIST['IDLE']\n",
    "        else:\n",
    "            if trend > 2:\n",
    "                # up\n",
    "                action = ACTION_LIST['BUY']\n",
    "            else:\n",
    "                # maintain the\n",
    "                action = ACTION_LIST['IDLE']\n",
    "        return action\n",
    "\n",
    "    def reaction(self, today, action):\n",
    "        today_price = self._DATA['open'][today]\n",
    "        if self.current_state == 0:\n",
    "            if action == ACTION_LIST['BUY']:\n",
    "                # Buy\n",
    "                print('----\\nTAKE ACTION {} in day{} \\n----'.format(action, today))\n",
    "                self.money -= today_price\n",
    "                self.current_state = 1\n",
    "            elif action == ACTION_LIST['IDLE']:\n",
    "                pass\n",
    "            else:\n",
    "                # short\n",
    "                print('----\\nTAKE ACTION {} in day{} \\n----'.format(action, today))\n",
    "                self.money += today_price\n",
    "                self.current_state = -1\n",
    "        elif self.current_state == 1:\n",
    "            if action == ACTION_LIST['SELL']:\n",
    "                print('----\\nTAKE ACTION {} in day{} \\n----'.format(action, today))\n",
    "                self.money += today_price\n",
    "                self.current_state = 0\n",
    "            elif action == ACTION_LIST['IDLE']:\n",
    "                pass\n",
    "            else:\n",
    "                raise StockValueError(ACTION_LIST['BUY'])\n",
    "        else:\n",
    "            if action == ACTION_LIST['BUY']:\n",
    "                print('----\\nTAKE ACTION {} in day{} \\n----'.format(action, today))\n",
    "                self.money -= today_price\n",
    "                self.current_state = 0\n",
    "            elif action == ACTION_LIST['IDLE']:\n",
    "                pass\n",
    "            else:\n",
    "                raise StockValueError(ACTION_LIST['SELL'])\n",
    "\n",
    "    def get_money(self):\n",
    "        return self.money\n",
    "\n",
    "    def get_today_price(self, today):\n",
    "        return self._DATA['open'][today]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trader.py\n",
    "\n",
    "在trader.py中，我們將會引入以上的各個模組`ac`, `stocktrader`, `env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Env\n",
    "from stocktrader import (StockTrader, ACTION_LIST)\n",
    "from ac import (Actor, Critic)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "MAX_EPISODE = 25\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n",
    "N_F = 5 # 3 feature as state\n",
    "N_A = 5 # 5 action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 並做一些初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--training TRAINING] [--testing TESTING]\n",
      "                             [--output OUTPUT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/kuoteng/Library/Jupyter/runtime/kernel-a0beccea-b639-474a-8023-7f1b7c122d39.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuoteng/.pyenv/versions/3.6.4/envs/NCKU-DSAI/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--training',\n",
    "                       default='training_data.csv',\n",
    "                       help='input training data file name')\n",
    "    parser.add_argument('--testing',\n",
    "                        default='testing_data.csv',\n",
    "                        help='input testing data file name')\n",
    "    parser.add_argument('--output',\n",
    "                        default='output.csv',\n",
    "                        help='output file name')\n",
    "    args = parser.parse_args()\n",
    "    training_data = pd.read_csv(args.training, names=['open', 'high', 'low', 'close'])\n",
    "    env = Env()\n",
    "    env.load_data(training_data)\n",
    "    env.training_preprocess() # compute the training data quantile\n",
    "\n",
    "    sess = tf.Session()\n",
    "\n",
    "    actor = Actor(sess, n_features=N_F, n_actions=N_A, lr=LR_A)\n",
    "    critic = Critic(sess, n_features=N_F, lr=LR_C)     # we need a good teacher, so the teacher should learn faster than the actor\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 進行學習與訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4421d68a1100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_EPISODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mlast_long_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_short_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# reset training_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrack_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "    for i_episode in range(MAX_EPISODE):\n",
    "        (last_long_avg, last_short_avg, state) = env.reset()\n",
    "        # reset training_env\n",
    "\n",
    "        track_r = []\n",
    "        total_action = [0,0,0,0,0]\n",
    "        error_count = [0, 0, 0]\n",
    "        env.error_count = [0,0,0]\n",
    "        error_index = [0,0,0,0,0]\n",
    "        error_index_2 = [0,0,0,0,0]\n",
    "        error_index_cumulator = []\n",
    "        for i in range(env.data_len()):\n",
    "\n",
    "            action = actor.choose_action(state)\n",
    "            (last_long_avg_, last_short_avg_, reward, state_) = env.step(i, action, state[2], state[0], last_long_avg, last_short_avg)\n",
    "\n",
    "            track_r.append(reward)\n",
    "\n",
    "            # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "            td_error = critic.learn(state, reward, state_)\n",
    "            # true_gradient = grad[logPi(s,a) * td_error]\n",
    "            actor.learn(state, action, td_error)\n",
    "            # XXX\n",
    "            if (state[2] - action > 1) or (state[2] - action < -1):\n",
    "                error_count[2] += 1\n",
    "                error_index[action] += 1\n",
    "                error_index_cumulator.append(action)\n",
    "            elif state[2] != action:\n",
    "                error_count[1] += 1\n",
    "                error_index_2[action] += 1\n",
    "            else:\n",
    "                error_count[0] += 1\n",
    "            total_action[action] +=1\n",
    "            #\n",
    "            state = state_\n",
    "            last_long_avg = last_long_avg_\n",
    "            last_short_avg = last_short_avg_\n",
    "\n",
    "\n",
    "        ep_rs_sum = sum(track_r)\n",
    "        print('the actions are \\n 0: {0},1: {1}, 2: {2}, 3: {3}, 4: {4} '.format(total_action[0],total_action[1],total_action[2], total_action[3], total_action[4]))\n",
    "        print('---\\n0: {}, +-<1: {}, +->1: {}\\n---'.format(error_count[0], error_count[1], error_count[2]))\n",
    "        print('reward list:\\n -100: {}, 10: {}, -50: {}'.format(env.error_count[0], env.error_count[1], env.error_count[2]))\n",
    "        print('most error is {}'.format(error_index))\n",
    "        print('less error is {}'.format(error_index_2))\n",
    "        print('cumulator of most error: ', error_index_cumulator)\n",
    "        print('---')\n",
    "        print(\"episode:\", i_episode, \"  reward:\", ep_rs_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用測試資料進行預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b6871a9fb5e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtesting_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'open'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'high'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'low'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStockTrader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "    # Testing\n",
    "    testing_data = pd.read_csv(args.testing, names=['open', 'high', 'low', 'close'])\n",
    "    output_file = open(args.output, 'w')\n",
    "    trader = StockTrader()\n",
    "    trader.load_data(testing_data)\n",
    "    env.load_data(testing_data)\n",
    "    (last_long_avg, last_short_avg, state) = env.reset()\n",
    "    error_count = [0,0,0]\n",
    "    error_index = [0,0,0,0,0]\n",
    "    error_index_2 = [0,0,0,0,0]\n",
    "    error_index_cumulator = []\n",
    "    for i in range(trader.data_len()):\n",
    "        if i > 0:\n",
    "            ## according the yesterday action\n",
    "            ## if i == data_len() - 1(269), it will be the 268's action to create a reaction\n",
    "            trader.reaction(i, predict_action)\n",
    "            output_file.write(str(predict_action) + '\\n')\n",
    "            print('Day {}: your money is {} | open: {} | action : {}'.format(i, trader.get_money(), trader.get_today_price(i), predict_action))\n",
    "\n",
    "        trend = actor.choose_action(state)\n",
    "        predict_action = trader.predict_action(trend, i)\n",
    "        (last_long_avg_, last_short_avg_, state_) = env.get_env(i, state[0], last_long_avg, last_short_avg)\n",
    "        state = state_\n",
    "        last_long_avg = last_long_avg_\n",
    "        last_short_avg = last_short_avg_\n",
    "\n",
    "        # XXX\n",
    "        if (state[2] - trend > 1) or (state[2] - trend < -1):\n",
    "            error_count[2] += 1\n",
    "            error_index[trend] += 1\n",
    "            error_index_cumulator.append(trend)\n",
    "        elif state[2] - trend != 0:\n",
    "            error_count[1] += 1\n",
    "            error_index_2[trend] += 1\n",
    "        else:\n",
    "            error_count[0] += 1\n",
    "    print('0: {}, +-<1: {}, +->1: {}'.format(error_count[0], error_count[1], error_count[2]))\n",
    "    print('most error is {}'.format(error_index))\n",
    "    print('less error is {}'.format(error_index_2))\n",
    "    print('cumulator of most error: ', error_index_cumulator)\n",
    "\n",
    "    output_file.close()\n",
    "\n",
    "    if trader.current_state == -1:\n",
    "        final_line = 'your final money is {}, the last close price is {}'.format(\n",
    "            trader.get_money() - testing_data['close'][trader.data_len() - 1],\n",
    "            testing_data['close'][trader.data_len() - 1])\n",
    "    elif trader.current_state == 1:\n",
    "        final_line = 'your final money is {}, the last close price is {}'.format(\n",
    "            trader.get_money() + testing_data['close'][trader.data_len() - 1],\n",
    "            testing_data['close'][trader.data_len() - 1])\n",
    "    else:\n",
    "        final_line = 'your final money is {}, the last close price is {}'.format(\n",
    "            trader.get_money(),\n",
    "            testing_data['close'][trader.data_len() - 1])\n",
    "    print(final_line)\n",
    "    print('the origin buy-and-hold is {}'.format((testing_data['close'][trader.data_len() - 1] - testing_data['open'][1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
